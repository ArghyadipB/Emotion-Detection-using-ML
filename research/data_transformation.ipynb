{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects\\\\E2E Emotion Detection from text\\\\Emotion-Detection-using-ML'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GOING BACK TO RIGHT DIRECTORY\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ML_emotion_detection.constants import *\n",
    "from src.ML_emotion_detection.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ML_emotion_detection import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from src.ML_emotion_detection.utils.common import save_bin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.tfidf = TfidfVectorizer(max_features=4000)\n",
    "\n",
    "    def train_test_spliting(self, test_size=0.2):\n",
    "        # Load data from parquet file\n",
    "        data = pd.read_parquet('E:/Projects/E2E Emotion Detection from text/Emotion-Detection-using-ML/artifacts/data_ingestion/train-00000-of-00001.parquet')\n",
    "\n",
    "        logger.info(\"Split data into training and test sets\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data['text'],\n",
    "                                                            data['label'],\n",
    "                                                            test_size=test_size,\n",
    "                                                            stratify=data['label'],\n",
    "                                                            random_state=42)\n",
    "\n",
    "        # Save y_train\n",
    "        save_bin(y_train, os.path.join(self.config.root_dir, \"y_train.joblib\"))\n",
    "\n",
    "        # Save y_test\n",
    "        save_bin(y_test, os.path.join(self.config.root_dir, \"y_test.joblib\"))\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def preprocess(self, text, *args):\n",
    "        # Preprocess the text (lowercasing, remove URLs, numbers, etc.)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub(r'#\\w+', '', text)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        # Tokenization and Lemmatization\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens if word not in self.stop_words]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def pos_count_features(self, text, *args):\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Get POS tags for the tokens\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "        # Define POS counts for common POS tags\n",
    "        pos_counts = {\n",
    "            'ADJ': 0, 'ADP': 0, 'ADV': 0, 'AUX': 0, 'CCONJ': 0, 'DET': 0,\n",
    "            'INTJ': 0, 'NOUN': 0, 'NUM': 0, 'PART': 0, 'PRON': 0, 'PROPN': 0,\n",
    "            'PUNCT': 0, 'SCONJ': 0, 'SYM': 0, 'VERB': 0, 'X': 0\n",
    "        }\n",
    "\n",
    "        # Loop through tagged tokens and count POS tags\n",
    "        for token, tag in tagged_tokens:\n",
    "            if tag.startswith('JJ'):  # Adjective (JJ, JJR, JJS)\n",
    "                pos_counts['ADJ'] += 1\n",
    "            elif tag.startswith('RB'):  # Adverb (RB, RBR, RBS)\n",
    "                pos_counts['ADV'] += 1\n",
    "            elif tag.startswith('VB'):  # Verb (VB, VBD, VBG, VBN, VBP, VBZ)\n",
    "                pos_counts['VERB'] += 1\n",
    "            elif tag.startswith('NN'):  # Noun (NN, NNS, NNP, NNPS)\n",
    "                pos_counts['NOUN'] += 1\n",
    "            elif tag == 'IN':  # Preposition (IN)\n",
    "                pos_counts['ADP'] += 1\n",
    "            elif tag == 'DT':  # Determiner (DT)\n",
    "                pos_counts['DET'] += 1\n",
    "            elif tag == 'PRP' or tag == 'PRP$':  # Pronoun (PRP, PRP$)\n",
    "                pos_counts['PRON'] += 1\n",
    "            elif tag == 'TO':  # \"to\" (particle or infinitive marker)\n",
    "                pos_counts['PART'] += 1\n",
    "            elif tag == 'PDT':  # Predeterminer (PDT)\n",
    "                pos_counts['DET'] += 1\n",
    "            elif tag == 'CD':  # Cardinal number (CD)\n",
    "                pos_counts['NUM'] += 1\n",
    "            elif tag == 'CC':  # Coordinating conjunction (CC)\n",
    "                pos_counts['CCONJ'] += 1\n",
    "            elif tag == 'RP':  # Particle (RP)\n",
    "                pos_counts['PART'] += 1\n",
    "            elif tag == ',':  # Punctuation (comma, period, etc.)\n",
    "                pos_counts['PUNCT'] += 1\n",
    "            elif tag == 'SYM':  # Symbol (SYM)\n",
    "                pos_counts['SYM'] += 1\n",
    "            elif tag == 'EX':  # Existential there (EX)\n",
    "                pos_counts['X'] += 1\n",
    "            else:\n",
    "                # For any unrecognized POS tags\n",
    "                pos_counts['X'] += 1\n",
    "\n",
    "        return pd.Series(pos_counts)\n",
    "\n",
    "    def text_feature_extraction(self):\n",
    "\n",
    "\n",
    "        text_column = 'text'\n",
    "        numerical_columns = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
    "              'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "        label_columns = 'label'\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('text', self.tfidf, text_column),  # Apply TF-IDF to the text column\n",
    "                ('num', StandardScaler(), numerical_columns)  # Apply StandardScaler to numerical columns\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "    def transform_and_save(self, preprocessor, X_train, X_test):\n",
    "        # Fit and transform the training data\n",
    "        X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "        # Transform the test data using the fitted preprocessor\n",
    "        X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "        # Save X_train\n",
    "        save_bin(X_train_processed, os.path.join(self.config.root_dir, \"X_train.joblib\"))\n",
    "\n",
    "        # Save X_test\n",
    "        save_bin(X_test_processed, os.path.join(self.config.root_dir, \"X_test.joblib\"))\n",
    "        \n",
    "        logger.info(f\"Training set shape after preprocessing: {X_train_processed.shape}\")\n",
    "        logger.info(f\"Test set shape after preprocessing: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-19 21:17:41,240: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-11-19 21:17:41,248: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-11-19 21:17:41,253: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-11-19 21:17:41,259: INFO: common: created directory at: artifacts]\n",
      "[2024-11-19 21:17:41,264: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2024-11-19 21:17:42,077: INFO: 1974438582: Split data into training and test sets]\n",
      "[2024-11-19 21:17:42,506: INFO: common: binary file saved at: artifacts/data_transformation\\y_train.joblib]\n",
      "[2024-11-19 21:17:42,623: INFO: common: binary file saved at: artifacts/data_transformation\\y_test.joblib]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 333447/333447 [02:43<00:00, 2043.10it/s]\n",
      "100%|██████████| 83362/83362 [00:33<00:00, 2491.82it/s]\n",
      "100%|██████████| 333447/333447 [13:10<00:00, 421.76it/s]\n",
      "100%|██████████| 83362/83362 [03:42<00:00, 374.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-19 21:38:16,206: INFO: common: binary file saved at: artifacts/data_transformation\\X_train.joblib]\n",
      "[2024-11-19 21:38:16,737: INFO: common: binary file saved at: artifacts/data_transformation\\X_test.joblib]\n",
      "[2024-11-19 21:38:16,739: INFO: 1974438582: Training set shape after preprocessing: (333447, 4017)]\n",
      "[2024-11-19 21:38:16,741: INFO: 1974438582: Test set shape after preprocessing: (83362, 4017)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "\n",
    "    # Step 1: Initialize the DataTransformation class\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "\n",
    "    # Step 2: Split the data into train and test\n",
    "    Tr_X, Te_X = data_transformation.train_test_spliting()\n",
    "\n",
    "    # Step 3: Preprocess and clean the text using progress_apply\n",
    "    Tr_X = Tr_X.progress_apply(data_transformation.preprocess)\n",
    "    Te_X = Te_X.progress_apply(data_transformation.preprocess)\n",
    "\n",
    "\n",
    "    # Step 4: Extract pos count features\n",
    "    Tr_pos_counts = Tr_X.progress_apply(data_transformation.pos_count_features)\n",
    "    Te_pos_counts = Te_X.progress_apply(data_transformation.pos_count_features)\n",
    "\n",
    "    # Step 4.1 concat preprocessed text and count of pos features\n",
    "    Tr_X_intermediate = pd.concat([Tr_X, Tr_pos_counts], axis=1)\n",
    "    Te_X_intermediate = pd.concat([Te_X, Te_pos_counts], axis=1)\n",
    "\n",
    "\n",
    "    # # Step 5: Create the ColumnTransformer instance\n",
    "    preprocessor = data_transformation.text_feature_extraction()\n",
    "\n",
    "    # Step 6: Transform and save the train test independent columns\n",
    "    data_transformation.transform_and_save(preprocessor, Tr_X_intermediate, Te_X_intermediate)\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
