{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects\\\\E2E Emotion Detection from text\\\\Emotion-Detection-using-ML'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GOING BACK TO RIGHT DIRECTORY\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ML_emotion_detection.constants import *\n",
    "from src.ML_emotion_detection.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ML_emotion_detection import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from src.ML_emotion_detection.utils.common import save_bin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom text preprocessing transformer for text data.\n",
    "\n",
    "    This class handles text cleaning, lowercasing, stop word removal, \n",
    "    and lemmatization.\n",
    "\n",
    "    Attributes:\n",
    "        stop_words (set): Set of English stop words.\n",
    "        lemmatizer (WordNetLemmatizer): Lemmatizer for reducing words to their base form.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the TextPreprocessor with stop words and lemmatizer.\n",
    "        \"\"\"\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method (does nothing as no fitting is required).\n",
    "\n",
    "        Args:\n",
    "            X (pd.Series): Input text data.\n",
    "            y (pd.Series, optional): Target labels. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            self: The fitted TextPreprocessor object.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Applies text preprocessing to the input data.\n",
    "\n",
    "        Args:\n",
    "            X (pd.Series): Input text data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Preprocessed text data with one column 'text'.\n",
    "        \"\"\"\n",
    "        processed = X.progress_apply(self._preprocess)\n",
    "        return pd.DataFrame(processed, columns=['text'])\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Preprocesses a single text input by cleaning, tokenizing, and lemmatizing.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Preprocessed text.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub(r'#\\w+', '', text)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens if word not in self.stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "\n",
    "class POSCountFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer to count parts-of-speech (POS) features.\n",
    "\n",
    "    Attributes:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method (does nothing as no fitting is required).\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input text data.\n",
    "            y (pd.Series, optional): Target labels. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            self: The fitted POSCountFeatures object.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Applies POS counting to the input text data.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input text data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Text data with additional POS count features.\n",
    "        \"\"\"\n",
    "        pos_count = X['text'].progress_apply(self._get_pos_counts)\n",
    "        return pd.concat([X, pos_count], axis=1)\n",
    "\n",
    "    def _get_pos_counts(self, text):\n",
    "        \"\"\"\n",
    "        Counts parts-of-speech (POS) in the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            pd.Series: Counts of different POS tags.\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "        pos_counts = {\n",
    "            'ADJ': 0, 'ADP': 0, 'ADV': 0, 'AUX': 0, 'CCONJ': 0, 'DET': 0,\n",
    "            'INTJ': 0, 'NOUN': 0, 'NUM': 0, 'PART': 0, 'PRON': 0, 'PROPN': 0,\n",
    "            'PUNCT': 0, 'SCONJ': 0, 'SYM': 0, 'VERB': 0, 'X': 0\n",
    "        }\n",
    "\n",
    "        for token, tag in tagged_tokens:\n",
    "            if tag.startswith('JJ'):\n",
    "                pos_counts['ADJ'] += 1\n",
    "            elif tag.startswith('RB'):\n",
    "                pos_counts['ADV'] += 1\n",
    "            elif tag.startswith('VB'):\n",
    "                pos_counts['VERB'] += 1\n",
    "            elif tag.startswith('NN'):\n",
    "                pos_counts['NOUN'] += 1\n",
    "            elif tag == 'IN':\n",
    "                pos_counts['ADP'] += 1\n",
    "            elif tag == 'DT':\n",
    "                pos_counts['DET'] += 1\n",
    "            elif tag == 'PRP' or tag == 'PRP$':\n",
    "                pos_counts['PRON'] += 1\n",
    "            elif tag == 'TO':\n",
    "                pos_counts['PART'] += 1\n",
    "            elif tag == 'PDT':\n",
    "                pos_counts['DET'] += 1\n",
    "            elif tag == 'CD':\n",
    "                pos_counts['NUM'] += 1\n",
    "            elif tag == 'CC':\n",
    "                pos_counts['CCONJ'] += 1\n",
    "            elif tag == 'RP':\n",
    "                pos_counts['PART'] += 1\n",
    "            elif tag == ',':\n",
    "                pos_counts['PUNCT'] += 1\n",
    "            elif tag == 'SYM':\n",
    "                pos_counts['SYM'] += 1\n",
    "            elif tag == 'EX':\n",
    "                pos_counts['X'] += 1\n",
    "            else:\n",
    "                pos_counts['X'] += 1\n",
    "\n",
    "        return pd.Series(pos_counts)\n",
    "\n",
    "\n",
    "class TfidfFeature:\n",
    "    \"\"\"\n",
    "    A feature transformer that combines TF-IDF features and scaled numerical features.\n",
    "\n",
    "    Attributes:\n",
    "        preprocessor (ColumnTransformer): Transformer for text and numerical columns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the TfidfFeature transformer with TF-IDF and numerical scaling.\n",
    "        \"\"\"\n",
    "        text_column = 'text'\n",
    "        numerical_columns = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
    "                             'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('text', TfidfVectorizer(max_features=4000), text_column),\n",
    "                ('num', StandardScaler(), numerical_columns)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fits the preprocessor to the input data.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input data with text and numerical features.\n",
    "            y (pd.Series, optional): Target labels. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            self: The fitted TfidfFeature object.\n",
    "        \"\"\"\n",
    "        self.preprocessor.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transforms the input data using the fitted preprocessor.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input data with text and numerical features.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Transformed data.\n",
    "        \"\"\"\n",
    "        return self.preprocessor.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fits the preprocessor and transforms the input data.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input data with text and numerical features.\n",
    "            y (pd.Series, optional): Target labels. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Transformed data.\n",
    "        \"\"\"\n",
    "        return self.preprocessor.fit_transform(X)\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    \"\"\"\n",
    "    A class for handling data transformation, including preprocessing, feature extraction,\n",
    "    and train-test splitting.\n",
    "\n",
    "    Attributes:\n",
    "        config (DataTransformationConfig): Configuration object.\n",
    "        stop_words (set): Set of English stop words.\n",
    "        lemmatizer (WordNetLemmatizer): Lemmatizer for text preprocessing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Initializes the DataTransformation object.\n",
    "\n",
    "        Args:\n",
    "            config (DataTransformationConfig): Configuration for data transformation.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def train_test_spliting(self, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Splits data into training and testing sets.\n",
    "\n",
    "        Args:\n",
    "            test_size (float): Proportion of data to be used for testing. Defaults to 0.2.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Training and testing data (X_train, X_test, y_train, y_test).\n",
    "        \"\"\"\n",
    "        data = pd.read_parquet('E:/Projects/E2E Emotion Detection from text/Emotion-Detection-using-ML/artifacts/data_ingestion/train-00000-of-00001.parquet')\n",
    "        logger.info(\"Split data into training and test sets\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data['text'],\n",
    "                                                            data['label'],\n",
    "                                                            test_size=test_size,\n",
    "                                                            stratify=data['label'],\n",
    "                                                            random_state=42)\n",
    "\n",
    "        save_bin(y_train, os.path.join(self.config.root_dir, \"y_train.joblib\"))\n",
    "        save_bin(y_test, os.path.join(self.config.root_dir, \"y_test.joblib\"))\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def pipeline_and_transform(self, X_train, X_test):\n",
    "        \"\"\"\n",
    "        Creates a preprocessing pipeline, transforms the data, and saves the outputs.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.Series): Training text data.\n",
    "            X_test (pd.Series): Testing text data.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Transformed training and testing data (X_train_processed, X_test_processed).\n",
    "        \"\"\"\n",
    "        pipeline = Pipeline([\n",
    "            ('text_preprocessor', TextPreprocessor()),\n",
    "            ('pos_counter', POSCountFeatures()),\n",
    "            ('tfidf_feature', TfidfFeature())\n",
    "        ])\n",
    "        \n",
    "        logger.info(\"Transforming train and test data\")\n",
    "        X_train_processed = pipeline.fit_transform(X_train)\n",
    "        X_test_processed = pipeline.transform(X_test)\n",
    "\n",
    "        save_bin(pipeline, os.path.join(self.config.root_dir, \"preprocessor.joblib\"))\n",
    "        save_bin(X_train_processed, os.path.join(self.config.root_dir, \"X_train.joblib\"))\n",
    "        save_bin(X_test_processed, os.path.join(self.config.root_dir, \"X_test.joblib\"))\n",
    "        \n",
    "        logger.info(f\"Training set shape after preprocessing: {X_train_processed.shape}\")\n",
    "        logger.info(f\"Test set shape after preprocessing: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-19 21:17:41,240: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-11-19 21:17:41,248: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-11-19 21:17:41,253: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-11-19 21:17:41,259: INFO: common: created directory at: artifacts]\n",
      "[2024-11-19 21:17:41,264: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2024-11-19 21:17:42,077: INFO: 1974438582: Split data into training and test sets]\n",
      "[2024-11-19 21:17:42,506: INFO: common: binary file saved at: artifacts/data_transformation\\y_train.joblib]\n",
      "[2024-11-19 21:17:42,623: INFO: common: binary file saved at: artifacts/data_transformation\\y_test.joblib]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 333447/333447 [02:43<00:00, 2043.10it/s]\n",
      "100%|██████████| 83362/83362 [00:33<00:00, 2491.82it/s]\n",
      "100%|██████████| 333447/333447 [13:10<00:00, 421.76it/s]\n",
      "100%|██████████| 83362/83362 [03:42<00:00, 374.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-19 21:38:16,206: INFO: common: binary file saved at: artifacts/data_transformation\\X_train.joblib]\n",
      "[2024-11-19 21:38:16,737: INFO: common: binary file saved at: artifacts/data_transformation\\X_test.joblib]\n",
      "[2024-11-19 21:38:16,739: INFO: 1974438582: Training set shape after preprocessing: (333447, 4017)]\n",
      "[2024-11-19 21:38:16,741: INFO: 1974438582: Test set shape after preprocessing: (83362, 4017)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    Tr_X, Te_X = data_transformation.train_test_spliting()\n",
    "    data_transformation.pipeline_and_transform(Tr_X, Te_X)\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
